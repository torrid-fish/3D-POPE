# 3D-POPE

Evaluation scripts for the 3D-POPE benchmark based on the official dataset.


## Installation
Install LEO (embeodied-generalist) and its dependencies by following the instructions in the [official repository](https://github.com/embodied-generalist/embodied-generalist)

```bash
conda create -n leo python=3.9 -y
conda activate leo
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
uv pip install -r embodied-generalist/requirements.txt
uv pip install peft==0.5.0 --no-deps
cd embodied-generalist/model/pointnetpp
python setup.py install # This may take a while
cd ../../..

# Download the PointNet++ weights pretrained on the VIL3DRef dataset
wget https://huggingface.co/datasets/huangjy-pku/LEO_data/resolve/main/pointnetpp_vil3dref.pth -O embodied-generalist/pointnetpp_vil3dref.pth

# Download the 3D-GRAND dataset with global alignment
wget https://huggingface.co/datasets/huangjy-pku/LEO_data/resolve/main/pcd_with_global_alignment.zip
unzip pcd_with_global_alignment.zip -d scan_data
rm pcd_with_global_alignment.zip

# Download pretrained LEO weights
mkdir -p embodied-generalist/sft_noact
wget https://huggingface.co/datasets/huangjy-pku/LEO_data/resolve/main/sft_noact.pth -O embodied-generalist/sft_noact/pytorch_model.bin
```

After this, make the following code modifications to fix compatibility issues:
1. Replace `from torch._six import string_classes` in [`embodied-generalist/common/misc.py`](embodied-generalist/common/misc.py) with `string_classes = str`. (related [issue](https://github.com/NVIDIA/apex/issues/1724#issuecomment-1846627576))
2. Comment out the imports of `PointNext` and `PointBERT` in [`embodied-generalist/model/pcd_backbone.py`](embodied-generalist/model/pcd_backbone.py) (Line 10 and 11).
3. Add `weights_only=False` to [`embodied-generalist/data/datasets.py`](embodied-generalist/data/datasets.py) in line 111.

## Execution (LEO)
Run the evaluation script as follows:

```bash
python eval_leo.py <PATH_TO_3D_POPE_JSON_FILE>
```

This will generate evaluation results and save them to a file named `results.json` under `eval_results/<3D_POPE_JSON_FILE>/probe/`.

## TODO
- [x] Inference LEO
- [x] Create the inference pipeline for 3D-POPE
- [ ] Calculate the score from the results.json file
- [ ] Test the effect of removing `<image>` tag from the prompt
- [ ] Inference other 3D-LLMs (e.g., LION, 3D-ChatGPT, etc.)

## About the 3D-POPE Benchmark

Here's the description of the 3D-POPE benchmark from the original paper:

> To systematically evaluate the hallucination behavior of 3D-LLMs, we introduce the 3D Polling-based Object Probing Evaluation (3D-POPE) benchmark. 3D-POPE is designed to assess a model's ability to accurately identify the presence or absence of objects in a given 3D scene.
> 
> ### Dataset
> To facilitate the 3D-POPE benchmark, we curate a dedicated dataset from the ScanNet dataset, utilizing the semantic classes from ScanNet200 . Specifically, we use the ScanNet validation set as the foundation for evaluating 3D-LLMs on the 3D-POPE benchmark.
>
> ### Benchmark design
> 3D-POPE consists of triples with: a 3D scene, a posed question, and a binary answer ("Yes" or "No") indicating the presence or absence of an object. For balance, we maintain a 1:1 ratio of existent to nonexistent objects when constructing these triples. For negative samples (i.e., nonexistent objects), we use the following three distinct sampling strategies that are designed to challenge the model's robustness and assess its susceptibility to different levels of object hallucination. Random Sampling: Nonexistent objects are randomly selected from the set of objects not present in the 3D scene. Popular Sampling: We select the top-k most frequent objects not present in the 3D scene, where k equals the number of objects currently in the scene. Adversarial Sampling: For each positively identified object in the scene, we rank objects that are not present and have not been used as negative samples based on their frequency of co-occurrence with the positive object in the training dataset. The highest-ranking co-occurring object is selected as the adversarial sample, which differs from the original POPE to avoid adversarial samples mirroring popular samples, as indoor scenes often contain similar objects.
>
> ### Metrics
> 3D-POPE metrics include Precision, Recall, F1 Score, Accuracy, and Yes (%). Precision and Recall assess the model's ability to correctly affirm the presence of objects and identify the absence of objects, respectively. Precision is particularly important as it indicates the proportion of non-existing objects generated by the 3D-LLMs. The F1 Score, combining Precision and Recall, balances the two and serves as the primary evaluation metric. Accuracy measures the proportion of correctly answered questions for "Yes" and "No" responses. Additionally, the Yes (%) metric reports the ratio of incorrect "Yes" responses to understand the model's tendencies regarding object hallucination.

## Citation

```bibtex
@article{yang2024grand,
  title={3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination},
  author={Yang, Jianing and Chen, Xuweiyi and Madaan, Nikhil and Iyengar, Madhavan and Qian, Shengyi and Fouhey, David F. and Chai, Joyce},
  journal={arXiv preprint arXiv:2406.05132},
  year={2024}
}
```

**Paper**: [3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination](https://arxiv.org/abs/2406.05132v3) (Yang et al., 2024)
